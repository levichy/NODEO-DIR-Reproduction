{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NODEO-DIR**\n",
        "A Neural Ordinary Differential Equation Based Optimization Framework for Deformable Image Registration.\n",
        "\n",
        "**Reproducibility Project Goals:**\n",
        "- reproduce table 1\n",
        "- reproduce figure 4 (map 3D to 2D)\n"
      ],
      "metadata": {
        "id": "oErjFV4KpEwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "km21dt0GCxQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Deep Learning/Reproducibility Project')"
      ],
      "metadata": {
        "id": "6uZR-6YU0y_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c7cc67c-4856-4a24-eb39-12e418d6053a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nibabel --upgrade\n",
        "!pip install torchio"
      ],
      "metadata": {
        "id": "Sn3m-AZwwT1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366d1f1f-14ed-4078-8926-6dc8f2b0a567"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Collecting nibabel\n",
            "  Downloading nibabel-5.1.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from nibabel) (1.22.4)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.10/dist-packages (from nibabel) (23.1)\n",
            "Installing collected packages: nibabel\n",
            "  Attempting uninstall: nibabel\n",
            "    Found existing installation: nibabel 3.0.2\n",
            "    Uninstalling nibabel-3.0.2:\n",
            "      Successfully uninstalled nibabel-3.0.2\n",
            "Successfully installed nibabel-5.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchio\n",
            "  Downloading torchio-0.18.91-py2.py3-none-any.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.8/172.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from torchio) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torchio) (1.10.1)\n",
            "Collecting SimpleITK!=2.0.*,!=2.1.1.1\n",
            "  Downloading SimpleITK-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchio) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.10/dist-packages (from torchio) (2.0.0+cu118)\n",
            "Requirement already satisfied: typer[all] in /usr/local/lib/python3.10/dist-packages (from torchio) (0.7.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from torchio) (4.6.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from torchio) (5.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.1->torchio) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1->torchio) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.1->torchio) (3.25.2)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->torchio) (1.14.1)\n",
            "Requirement already satisfied: packaging>=17 in /usr/local/lib/python3.10/dist-packages (from nibabel->torchio) (23.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]->torchio) (8.1.3)\n",
            "Collecting shellingham<2.0.0,>=1.3.0\n",
            "  Downloading shellingham-1.5.0.post1-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting colorama<0.5.0,>=0.4.3\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting rich<13.0.0,>=10.11.0\n",
            "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich<13.0.0,>=10.11.0->typer[all]->torchio) (2.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.1->torchio) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.1->torchio) (1.3.0)\n",
            "Installing collected packages: SimpleITK, commonmark, shellingham, rich, Deprecated, colorama, torchio\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.3.4\n",
            "    Uninstalling rich-13.3.4:\n",
            "      Successfully uninstalled rich-13.3.4\n",
            "Successfully installed Deprecated-1.2.13 SimpleITK-2.2.1 colorama-0.4.6 commonmark-0.9.1 rich-12.6.0 shellingham-1.5.0.post1 torchio-0.18.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss"
      ],
      "metadata": {
        "id": "Sf5tQcRzqQFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NCC(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    NCC with cumulative sum implementation for acceleration. local (over window) normalized cross correlation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, win=21, eps=1e-5):\n",
        "        super(NCC, self).__init__()\n",
        "        self.eps = eps\n",
        "        self.win = win\n",
        "        self.win_raw = win\n",
        "\n",
        "    def window_sum_cs3D(self, I, win_size):\n",
        "        half_win = int(win_size / 2)\n",
        "        pad = [half_win + 1, half_win] * 3\n",
        "\n",
        "        I_padded = F.pad(I, pad=pad, mode='constant', value=0)  # [x+pad, y+pad, z+pad]\n",
        "\n",
        "        # Run the cumulative sum across all 3 dimensions\n",
        "        I_cs_x = torch.cumsum(I_padded, dim=2)\n",
        "        I_cs_xy = torch.cumsum(I_cs_x, dim=3)\n",
        "        I_cs_xyz = torch.cumsum(I_cs_xy, dim=4)\n",
        "\n",
        "        x, y, z = I.shape[2:]\n",
        "\n",
        "        # Use subtraction to calculate the window sum\n",
        "        I_win = I_cs_xyz[:, :, win_size:, win_size:, win_size:] \\\n",
        "                - I_cs_xyz[:, :, win_size:, win_size:, :z] \\\n",
        "                - I_cs_xyz[:, :, win_size:, :y, win_size:] \\\n",
        "                - I_cs_xyz[:, :, :x, win_size:, win_size:] \\\n",
        "                + I_cs_xyz[:, :, win_size:, :y, :z] \\\n",
        "                + I_cs_xyz[:, :, :x, win_size:, :z] \\\n",
        "                + I_cs_xyz[:, :, :x, :y, win_size:] \\\n",
        "                - I_cs_xyz[:, :, :x, :y, :z]\n",
        "\n",
        "        return I_win\n",
        "\n",
        "    def forward(self, I, J):\n",
        "        # compute CC squares\n",
        "        I = I.double()\n",
        "        J = J.double()\n",
        "\n",
        "        I2 = I * I\n",
        "        J2 = J * J\n",
        "        IJ = I * J\n",
        "\n",
        "        # compute local sums via cumsum trick\n",
        "        I_sum_cs = self.window_sum_cs3D(I, self.win)\n",
        "        J_sum_cs = self.window_sum_cs3D(J, self.win)\n",
        "        I2_sum_cs = self.window_sum_cs3D(I2, self.win)\n",
        "        J2_sum_cs = self.window_sum_cs3D(J2, self.win)\n",
        "        IJ_sum_cs = self.window_sum_cs3D(IJ, self.win)\n",
        "\n",
        "        win_size_cs = (self.win * 1.) ** 3\n",
        "\n",
        "        u_I_cs = I_sum_cs / win_size_cs\n",
        "        u_J_cs = J_sum_cs / win_size_cs\n",
        "\n",
        "        cross_cs = IJ_sum_cs - u_J_cs * I_sum_cs - u_I_cs * J_sum_cs + u_I_cs * u_J_cs * win_size_cs\n",
        "        I_var_cs = I2_sum_cs - 2 * u_I_cs * I_sum_cs + u_I_cs * u_I_cs * win_size_cs\n",
        "        J_var_cs = J2_sum_cs - 2 * u_J_cs * J_sum_cs + u_J_cs * u_J_cs * win_size_cs\n",
        "\n",
        "        cc_cs = cross_cs * cross_cs / (I_var_cs * J_var_cs + self.eps)\n",
        "        cc2 = cc_cs  # cross correlation squared\n",
        "\n",
        "        # return negative cc.\n",
        "        return 1. - torch.mean(cc2).float()\n",
        "\n",
        "def JacboianDet(J):\n",
        "    if J.size(-1) != 3:\n",
        "        J = J.permute(0, 2, 3, 4, 1)\n",
        "    J = J + 1\n",
        "    J = J / 2.\n",
        "    scale_factor = torch.tensor([J.size(1), J.size(2), J.size(3)]).to(J).view(1, 1, 1, 1, 3) * 1.\n",
        "    J = J * scale_factor\n",
        "\n",
        "    dy = J[:, 1:, :-1, :-1, :] - J[:, :-1, :-1, :-1, :]\n",
        "    dx = J[:, :-1, 1:, :-1, :] - J[:, :-1, :-1, :-1, :]\n",
        "    dz = J[:, :-1, :-1, 1:, :] - J[:, :-1, :-1, :-1, :]\n",
        "\n",
        "    Jdet0 = dx[:, :, :, :, 0] * (dy[:, :, :, :, 1] * dz[:, :, :, :, 2] - dy[:, :, :, :, 2] * dz[:, :, :, :, 1])\n",
        "    Jdet1 = dx[:, :, :, :, 1] * (dy[:, :, :, :, 0] * dz[:, :, :, :, 2] - dy[:, :, :, :, 2] * dz[:, :, :, :, 0])\n",
        "    Jdet2 = dx[:, :, :, :, 2] * (dy[:, :, :, :, 0] * dz[:, :, :, :, 1] - dy[:, :, :, :, 1] * dz[:, :, :, :, 0])\n",
        "\n",
        "    Jdet = Jdet0 - Jdet1 + Jdet2\n",
        "    return Jdet\n",
        "\n",
        "def neg_Jdet_loss(J):\n",
        "    Jdet = JacboianDet(J)\n",
        "    neg_Jdet = -1.0 * (Jdet - 0.5)\n",
        "    selected_neg_Jdet = F.relu(neg_Jdet)\n",
        "    return torch.mean(selected_neg_Jdet ** 2)\n",
        "\n",
        "def smoothloss_loss(df):\n",
        "    return (((df[:, :, 1:, :, :] - df[:, :, :-1, :, :]) ** 2).mean() + \\\n",
        "     ((df[:, :, :, 1:, :] - df[:, :, :, :-1, :]) ** 2).mean() + \\\n",
        "     ((df[:, :, :, :, 1:] - df[:, :, :, :, :-1]) ** 2).mean())\n",
        "\n",
        "def magnitude_loss(all_v):\n",
        "    all_v_x_2 = all_v[:, :, 0, :, :, :] * all_v[:, :, 0, :, :, :]\n",
        "    all_v_y_2 = all_v[:, :, 1, :, :, :] * all_v[:, :, 1, :, :, :]\n",
        "    all_v_z_2 = all_v[:, :, 2, :, :, :] * all_v[:, :, 2, :, :, :]\n",
        "    all_v_magnitude = torch.mean(all_v_x_2 + all_v_y_2 + all_v_z_2)\n",
        "    return all_v_magnitude"
      ],
      "metadata": {
        "id": "AwaEwl6OqcgU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural ODE"
      ],
      "metadata": {
        "id": "epUhqi6nq0Ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "\n",
        "def RK(z0, n_steps, f, h):\n",
        "    '''\n",
        "    4th Order Runge Kutta Numerical Solver\n",
        "    Input:\n",
        "      z0: initial condition\n",
        "      t0: initial time (not actual time, but the index of time)\n",
        "      n_steps: the number of steps to integrate\n",
        "      f: vector field\n",
        "      h: step size\n",
        "    Return:\n",
        "      z: the state after n_steps\n",
        "    '''\n",
        "    z = z0\n",
        "    for i in range(int(n_steps)):\n",
        "        k1 = h * f(z)\n",
        "        k2 = h * f(z + 0.5 * k1)\n",
        "        k3 = h * f(z + 0.5 * k2)\n",
        "        k4 = h * f(z + k3)\n",
        "        z = z + (1.0 / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n",
        "    return z\n",
        "\n",
        "def Euler(z0, n_steps, f, step_size):\n",
        "    '''\n",
        "    Simplest Euler ODE initial value solver\n",
        "    Input:\n",
        "      z0: initial condition\n",
        "      t0: initial time (not actual time, but the index of time)\n",
        "      n_steps: the number of steps to integrate\n",
        "      f: input phi -> output vector field\n",
        "      h: step size\n",
        "    Return:\n",
        "      z: the state after n_steps\n",
        "    '''\n",
        "    z = z0\n",
        "    for i_step in range(int(n_steps)):\n",
        "        z = z + step_size * f(z)\n",
        "    return z\n",
        "\n",
        "class ODEF(nn.Module):\n",
        "    def forward_with_grad(self, z, grad_outputs):\n",
        "        \"\"\"Compute f and a df/dz, a df/dp, a df/dt\"\"\"\n",
        "        batch_size = z.shape[0]\n",
        "        out = self.forward(z)\n",
        "\n",
        "        a = grad_outputs\n",
        "        adfdz, *adfdp = torch.autograd.grad(\n",
        "            # concatenating tuples\n",
        "            (out,), (z,) + tuple(self.parameters()), grad_outputs=(a),\n",
        "            allow_unused=True, retain_graph=True\n",
        "        )\n",
        "        # grad method automatically sums gradients for batch items, we have to expand them back\n",
        "        if adfdp is not None:\n",
        "            adfdp = torch.cat([p_grad.flatten() for p_grad in adfdp]).unsqueeze(\n",
        "                0)  # unsqueeze(0) add dimension 1 to the position 0\n",
        "            adfdp = adfdp.expand(batch_size, -1) / batch_size  # passing -1 does not change dimension in that position\n",
        "        return out, adfdz, adfdp\n",
        "\n",
        "    def flatten_parameters(self):\n",
        "        p_shapes = []\n",
        "        flat_parameters = []\n",
        "        for p in self.parameters():\n",
        "            p_shapes.append(p.size())\n",
        "            flat_parameters.append(p.flatten())\n",
        "        return torch.cat(flat_parameters)\n",
        "\n",
        "\n",
        "class ODEAdjoint(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, z0, t, flat_parameters, func, ode_solve, STEP_SIZE):\n",
        "        assert isinstance(func, ODEF)\n",
        "        bs, *z_shape = z0.size()\n",
        "        time_len = t.size(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # initialize z to len of time and type of z0\n",
        "            z = torch.zeros(time_len, bs, *z_shape).to(z0)\n",
        "            z[0] = z0\n",
        "            # solving throughout time\n",
        "            for i_t in range(time_len - 1):\n",
        "                # z0 updated to next step\n",
        "                z0 = ode_solve(z0, torch.abs(t[i_t + 1] - t[i_t]), func, STEP_SIZE)\n",
        "                z[i_t + 1] = z0\n",
        "\n",
        "        ctx.func = func\n",
        "        ctx.save_for_backward(t, z.clone(), flat_parameters)\n",
        "        ctx.ode_solve = ode_solve\n",
        "        ctx.STEP_SIZE = STEP_SIZE\n",
        "        return z\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dLdz):\n",
        "        \"\"\"\n",
        "        dLdz shape: time_len, batch_size, *z_shape\n",
        "        \"\"\"\n",
        "        func = ctx.func\n",
        "        t, z, flat_parameters = ctx.saved_tensors\n",
        "        time_len, bs, *z_shape = z.size()\n",
        "        n_dim = np.prod(z_shape)\n",
        "        n_params = flat_parameters.size(0)\n",
        "        ode_solve = ctx.ode_solve\n",
        "        STEP_SIZE = ctx.STEP_SIZE\n",
        "\n",
        "        # Dynamics of augmented system to be calculated backwards in time\n",
        "        def augmented_dynamics(aug_z_i):\n",
        "            \"\"\"\n",
        "            tensors here are temporal slices\n",
        "            t_i - is tensor with size: bs, 1\n",
        "            aug_z_i - is tensor with size: bs, n_dim*2 + n_params + 1\n",
        "            \"\"\"\n",
        "            z_i, a = aug_z_i[:, :n_dim], aug_z_i[:, n_dim:2 * n_dim]  # ignore parameters and time\n",
        "            # Unflatten z and a\n",
        "            z_i = z_i.view(bs, *z_shape)\n",
        "            a = a.view(bs, *z_shape)\n",
        "\n",
        "            with torch.set_grad_enabled(True):\n",
        "                z_i = z_i.detach().requires_grad_(True)\n",
        "                func_eval, adfdz, adfdp = func.forward_with_grad(z_i, grad_outputs=a)  # bs, *z_shape\n",
        "                adfdz = adfdz.to(z_i) if adfdz is not None else torch.zeros(bs, *z_shape).to(z_i)\n",
        "                adfdp = adfdp.to(z_i) if adfdp is not None else torch.zeros(bs, n_params).to(z_i)\n",
        "\n",
        "            # Flatten f and adfdz\n",
        "            func_eval = func_eval.view(bs, n_dim)\n",
        "            adfdz = adfdz.view(bs, n_dim)\n",
        "            return torch.cat((func_eval, -adfdz, -adfdp), dim=1)\n",
        "\n",
        "        dLdz = dLdz.view(time_len, bs, n_dim)  # flatten dLdz for convenience\n",
        "\n",
        "        with torch.no_grad():\n",
        "            ## Create placeholders for output gradients\n",
        "            # Prev computed backwards adjoints to be adjusted by direct gradients\n",
        "            adj_z = torch.zeros(bs, n_dim).to(dLdz)\n",
        "            adj_p = torch.zeros(bs, n_params).to(dLdz)\n",
        "            # In contrast to z and p we need to return gradients for all times\n",
        "            # adj_t = torch.zeros(time_len, bs, 1).to(dLdz)\n",
        "            for i_t in range(time_len - 1, 0, -1):\n",
        "                z_i = z[i_t]\n",
        "                t_i = t[i_t]\n",
        "                # f_i = func(z_i).view(bs, n_dim)\n",
        "                # Compute direct gradients\n",
        "                dLdz_i = dLdz[i_t]\n",
        "                # Adjusting adjoints with direct gradients\n",
        "                adj_z += dLdz_i\n",
        "\n",
        "                # Pack augmented variable\n",
        "                aug_z = torch.cat((z_i.view(bs, n_dim), adj_z, torch.zeros(bs, n_params).to(z)), dim=-1)\n",
        "\n",
        "                # Solve augmented system backwards\n",
        "                aug_ans = ode_solve(aug_z, torch.abs(t_i - t[i_t - 1]), augmented_dynamics, -STEP_SIZE)\n",
        "\n",
        "                # Unpack solved backwards augmented system\n",
        "                adj_z[:] = aug_ans[:, n_dim:2 * n_dim]\n",
        "                adj_p[:] += aug_ans[:, 2 * n_dim:2 * n_dim + n_params]\n",
        "\n",
        "                del aug_z, aug_ans\n",
        "\n",
        "            ## Adjust 0 time adjoint with direct gradients\n",
        "            # Compute direct gradients\n",
        "            dLdz_0 = dLdz[0]\n",
        "\n",
        "            # Adjust adjoints\n",
        "            adj_z += dLdz_0\n",
        "        return adj_z.view(bs, *z_shape), None, adj_p, None, None, None\n",
        "\n",
        "\n",
        "class NeuralODE(nn.Module):\n",
        "    def __init__(self, func, ode_solve, STEP_SIZE):\n",
        "        super(NeuralODE, self).__init__()\n",
        "        assert isinstance(func, ODEF)\n",
        "        self.func = func\n",
        "        self.ode_solve = Euler if ode_solve == 'Euler' else RK\n",
        "        self.STEP_SIZE = STEP_SIZE\n",
        "\n",
        "    def forward(self, z0, t=Tensor([0., 1.]), return_whole_sequence=False):\n",
        "        t = t.to(z0)\n",
        "        z = ODEAdjoint.apply(z0, t, self.func.flatten_parameters(), self.func, self.ode_solve, self.STEP_SIZE)\n",
        "        if return_whole_sequence:\n",
        "            return z\n",
        "        else:\n",
        "            return z[-1]"
      ],
      "metadata": {
        "id": "6qvbSGlFrDKB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network"
      ],
      "metadata": {
        "id": "PkpWmjqCqtan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "#from NeuralODE import ODEF\n",
        "\n",
        "\n",
        "class GaussianKernel(torch.nn.Module):\n",
        "    def __init__(self, win=11, nsig=0.1):\n",
        "        super(GaussianKernel, self).__init__()\n",
        "        self.win = win\n",
        "        self.nsig = nsig\n",
        "        kernel_x, kernel_y, kernel_z = self.gkern1D_xyz(self.win, self.nsig)\n",
        "        kernel = kernel_x * kernel_y * kernel_z\n",
        "        self.register_buffer(\"kernel_x\", kernel_x)\n",
        "        self.register_buffer(\"kernel_y\", kernel_y)\n",
        "        self.register_buffer(\"kernel_z\", kernel_z)\n",
        "        self.register_buffer(\"kernel\", kernel)\n",
        "\n",
        "    def gkern1D(self, kernlen=None, nsig=None):\n",
        "        '''\n",
        "        :param nsig: large nsig gives more freedom(pixels as agents), small nsig is more fluid.\n",
        "        :return: Returns a 1D Gaussian kernel.\n",
        "        '''\n",
        "        x = np.linspace(-nsig, nsig, kernlen + 1)\n",
        "        kern1d = np.diff(st.norm.cdf(x))\n",
        "        kern1d = kern1d / kern1d.sum()\n",
        "        return torch.tensor(kern1d, requires_grad=False).float()\n",
        "\n",
        "    def gkern1D_xyz(self, kernlen=None, nsig=None):\n",
        "        \"\"\"Returns 3 1D Gaussian kernel on xyz direction.\"\"\"\n",
        "        kernel_1d = self.gkern1D(kernlen, nsig)\n",
        "        kernel_x = kernel_1d.view(1, 1, -1, 1, 1)\n",
        "        kernel_y = kernel_1d.view(1, 1, 1, -1, 1)\n",
        "        kernel_z = kernel_1d.view(1, 1, 1, 1, -1)\n",
        "        return kernel_x, kernel_y, kernel_z\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = int((self.win - 1) / 2)\n",
        "        # Apply Gaussian by 3D kernel\n",
        "        x = F.conv3d(x, self.kernel, padding=pad)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AveragingKernel(torch.nn.Module):\n",
        "    def __init__(self, win=11):\n",
        "        super(AveragingKernel, self).__init__()\n",
        "        self.win = win\n",
        "\n",
        "    def window_averaging(self, v):\n",
        "        win_size = self.win\n",
        "        v = v.double()\n",
        "\n",
        "        half_win = int(win_size / 2)\n",
        "        pad = [half_win + 1, half_win] * 3\n",
        "\n",
        "        v_padded = F.pad(v, pad=pad, mode='constant', value=0)  # [x+pad, y+pad, z+pad]\n",
        "\n",
        "        # Run the cumulative sum across all 3 dimensions\n",
        "        v_cs_x = torch.cumsum(v_padded, dim=2)\n",
        "        v_cs_xy = torch.cumsum(v_cs_x, dim=3)\n",
        "        v_cs_xyz = torch.cumsum(v_cs_xy, dim=4)\n",
        "\n",
        "        x, y, z = v.shape[2:]\n",
        "\n",
        "        # Use subtraction to calculate the window sum\n",
        "        v_win = v_cs_xyz[:, :, win_size:, win_size:, win_size:] \\\n",
        "                - v_cs_xyz[:, :, win_size:, win_size:, :z] \\\n",
        "                - v_cs_xyz[:, :, win_size:, :y, win_size:] \\\n",
        "                - v_cs_xyz[:, :, :x, win_size:, win_size:] \\\n",
        "                + v_cs_xyz[:, :, win_size:, :y, :z] \\\n",
        "                + v_cs_xyz[:, :, :x, win_size:, :z] \\\n",
        "                + v_cs_xyz[:, :, :x, :y, win_size:] \\\n",
        "                - v_cs_xyz[:, :, :x, :y, :z]\n",
        "\n",
        "        # Normalize by number of elements\n",
        "        v_win = v_win / (win_size ** 3)\n",
        "        v_win = v_win.float()\n",
        "        return v_win\n",
        "\n",
        "    def forward(self, v):\n",
        "        return self.window_averaging(v)\n",
        "\n",
        "\n",
        "class BrainNet(ODEF):\n",
        "    def __init__(self, img_sz, smoothing_kernel, smoothing_win, smoothing_pass, ds, bs):\n",
        "        super(BrainNet, self).__init__()\n",
        "        padding_mode = 'replicate'\n",
        "        bias = True\n",
        "        self.ds = ds\n",
        "        self.bs = bs\n",
        "        self.img_sz = img_sz\n",
        "        self.smoothing_kernel = smoothing_kernel\n",
        "        self.smoothing_pass = smoothing_pass\n",
        "\n",
        "        # Define convolutional layers\n",
        "        self.enc_conv1 = nn.Conv3d(3, 32, kernel_size=3, stride=2, padding=1, padding_mode=padding_mode, bias=bias)\n",
        "        self.enc_conv2 = nn.Conv3d(32, 32, kernel_size=3, stride=2, padding=1, padding_mode=padding_mode, bias=bias)\n",
        "        self.enc_conv3 = nn.Conv3d(32, 32, kernel_size=3, stride=2, padding=1, padding_mode=padding_mode, bias=bias)\n",
        "        self.enc_conv4 = nn.Conv3d(32, 32, kernel_size=3, stride=2, padding=1, padding_mode=padding_mode, bias=bias)\n",
        "        self.enc_conv5 = nn.Conv3d(32, 32, kernel_size=3, stride=2, padding=1, padding_mode=padding_mode, bias=bias)\n",
        "\n",
        "        # Calculate bottleneck size\n",
        "        self.bottleneck_sz = int(\n",
        "            math.ceil(img_sz[0] / pow(2, self.ds)) * math.ceil(img_sz[1] / pow(2, self.ds)) * math.ceil(\n",
        "                img_sz[2] / pow(2, self.ds)))\n",
        "        \n",
        "        # Activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Calculate the number of in-features for the linear layer dynamically based on input size\n",
        "        self.in_features = self.calculate_in_features()\n",
        "\n",
        "        # Linear layers\n",
        "        self.lin1 = nn.Linear(self.in_features, self.bs, bias=bias)\n",
        "        self.lin2 = nn.Linear(self.bs, self.bottleneck_sz * 3, bias=bias)\n",
        "\n",
        "        # Create smoothing kernels\n",
        "        if self.smoothing_kernel == 'AK':\n",
        "            self.sk = AveragingKernel(win=smoothing_win)\n",
        "        else:\n",
        "            self.sk = GaussianKernel(win=smoothing_win, nsig=0.1)\n",
        "\n",
        "    def calculate_in_features(self):\n",
        "        \"\"\"\n",
        "        Calculates the number of in-features for the linear layer dynamically based on input size.\n",
        "        \"\"\"\n",
        "        x = torch.randn(1, 3, self.img_sz[0], self.img_sz[1], self.img_sz[2])  # Create a dummy input tensor\n",
        "        x = F.interpolate(x, scale_factor=0.5, mode='trilinear')  # Optional to downsample the image\n",
        "        x = self.relu(self.enc_conv1(x))\n",
        "        x = self.relu(self.enc_conv2(x))\n",
        "        x = self.relu(self.enc_conv3(x))\n",
        "        x = self.relu(self.enc_conv4(x))\n",
        "        x = self.enc_conv5(x)\n",
        "        return x.view(x.size(0), -1).size(1)  # Flatten the tensor and return the number of features\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Image dimensions\n",
        "        imgx = self.img_sz[0]\n",
        "        imgy = self.img_sz[1]\n",
        "        imgz = self.img_sz[2]\n",
        "\n",
        "        # Optional to downsample the image\n",
        "        x = F.interpolate(x, scale_factor=0.5, mode='trilinear')\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = self.relu(self.enc_conv1(x))\n",
        "        x = self.relu(self.enc_conv2(x))\n",
        "        x = self.relu(self.enc_conv3(x))\n",
        "        x = self.relu(self.enc_conv4(x))\n",
        "        x = self.enc_conv5(x)\n",
        "        \n",
        "        # Flatten tensor and apply linear layers\n",
        "        x = x.view(-1)\n",
        "        x = self.relu(self.lin1(x))\n",
        "        x = self.lin2(x)\n",
        "\n",
        "        # Reshape\n",
        "        x = x.view(1, 3, int(math.ceil(imgx / pow(2, self.ds))), int(math.ceil(imgy / pow(2, self.ds))),\n",
        "                   int(math.ceil(imgz / pow(2, self.ds))))\n",
        "        \n",
        "        # Upsampling\n",
        "        for _ in range(self.ds):\n",
        "            x = F.interpolate(x, scale_factor=2, mode='trilinear')\n",
        "\n",
        "        # Apply smoothing\n",
        "        for _ in range(self.smoothing_pass):\n",
        "            if self.smoothing_kernel == 'AK': # Averaging Kernel\n",
        "                x = self.sk(x)\n",
        "            else: # Gaussian Kernel\n",
        "                x_x = self.sk(x[:, 0, :, :, :].unsqueeze(1))\n",
        "                x_y = self.sk(x[:, 1, :, :, :].unsqueeze(1))\n",
        "                x_z = self.sk(x[:, 2, :, :, :].unsqueeze(1))\n",
        "                x = torch.cat([x_x, x_y, x_z], 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6EU-PRyXq_4p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "3ytmbQuxq5z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nibabel as nib\n",
        "#from Loss import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    N-D Spatial Transformer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, mode='bilinear'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        # create sampling grid\n",
        "        vectors = [torch.arange(0, s) for s in size]\n",
        "        grids = torch.meshgrid(vectors)\n",
        "        grid = torch.stack(grids)\n",
        "        grid = torch.unsqueeze(grid, 0)\n",
        "        grid = grid.type(torch.FloatTensor)\n",
        "\n",
        "        # registering the grid as a buffer cleanly moves it to the GPU, but it also\n",
        "        # adds it to the state dict. this is annoying since everything in the state dict\n",
        "        # is included when saving weights to disk, so the model files are way bigger\n",
        "        # than they need to be. so far, there does not appear to be an elegant solution.\n",
        "        # see: https://discuss.pytorch.org/t/how-to-register-buffer-without-polluting-state-dict\n",
        "        self.register_buffer('grid', grid)\n",
        "\n",
        "    def forward(self, src, flow, return_phi=False):\n",
        "        # new locations\n",
        "        new_locs = self.grid + flow\n",
        "        shape = flow.shape[2:]\n",
        "\n",
        "        # need to normalize grid values to [-1, 1] for resampler\n",
        "        for i in range(len(shape)):\n",
        "            new_locs[:, i, ...] = 2 * (new_locs[:, i, ...] / (shape[i] - 1) - 0.5)\n",
        "\n",
        "        # move channels dim to last position\n",
        "        # grid_sample function of PyTorch expects the spatial dimensions to be in the last dimensions of the tensor\n",
        "        if len(shape) == 2:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 1) # pytorch grid_sample expects (batch, height, width, dim)\n",
        "            new_locs = new_locs[..., [1, 0]] # pytorch expects (y, x)\n",
        "        elif len(shape) == 3:\n",
        "            new_locs = new_locs.permute(0, 2, 3, 4, 1) # pytorch grid_sample expects (batch, depth, height, width, dim)\n",
        "            new_locs = new_locs[..., [2, 1, 0]] # pytorch expects (z, y, x)\n",
        "\n",
        "        if return_phi:\n",
        "            return F.grid_sample(src, new_locs, align_corners=True, mode=self.mode), new_locs\n",
        "        else:\n",
        "            return F.grid_sample(src, new_locs, align_corners=True, mode=self.mode)\n",
        "\n",
        "def load_nii(path):\n",
        "    \"\"\"\n",
        "    Load a NIfTI file from disk and extract the data array and affine transformation matrix\n",
        "    \"\"\"\n",
        "    X = nib.load(path)\n",
        "    affine = X.affine\n",
        "    X = X.get_fdata()\n",
        "    return X, affine\n",
        "\n",
        "def save_nii(img, savename, affine):\n",
        "    \"\"\"\n",
        "    Save a NIfTI file to disk\n",
        "    \"\"\"\n",
        "    new_img = nib.nifti1.Nifti1Image(img, affine, header=None)\n",
        "    nib.save(new_img, savename)\n",
        "\n",
        "def generate_grid3D_tensor(shape):\n",
        "    \"\"\"\n",
        "    Generate a 3D grid tensor of the specified shape.\n",
        "    \"\"\"\n",
        "    x_grid = torch.linspace(-1., 1., shape[0])\n",
        "    y_grid = torch.linspace(-1., 1., shape[1])\n",
        "    z_grid = torch.linspace(-1., 1., shape[2])\n",
        "    x_grid, y_grid, z_grid = torch.meshgrid(x_grid, y_grid, z_grid)\n",
        "\n",
        "    # Note that default the dimension in the grid is reversed:\n",
        "    # z, y, x\n",
        "    grid = torch.stack([z_grid, y_grid, x_grid], dim=0)\n",
        "    return grid\n",
        "\n",
        "def dice(array1, array2, labels):\n",
        "    \"\"\"\n",
        "    Computes the dice overlap between two arrays for a given set of integer labels.\n",
        "    \"\"\"\n",
        "    dicem = np.zeros(len(labels))\n",
        "    for idx, label in enumerate(labels):\n",
        "        top = 2 * np.sum(np.logical_and(array1 == label, array2 == label))\n",
        "        bottom = np.sum(array1 == label) + np.sum(array2 == label)\n",
        "        bottom = np.maximum(bottom, np.finfo(float).eps)  # add epsilon\n",
        "        dicem[idx] = top / bottom\n",
        "    return dicem"
      ],
      "metadata": {
        "id": "TKh62YOXrHqQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Registration"
      ],
      "metadata": {
        "id": "iG1_r0BZqm4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import types\n",
        "#from Network import BrainNet\n",
        "#from Loss import *\n",
        "#from NeuralODE import *\n",
        "#from Utils import *\n",
        "\n",
        "def main(config, dice_list):\n",
        "    # Specify device and load images and their affine transformation matrices\n",
        "    device = torch.device(config.device)\n",
        "    fixed, affine_fixed = load_nii(config.fixed)\n",
        "    moving, affine_moving = load_nii(config.moving)\n",
        "    \n",
        "    # Check that the fixed and moving images have the same size and affine transformation matrix\n",
        "    assert fixed.shape == moving.shape\n",
        "    assert (affine_fixed == affine_moving).all()\n",
        "    config.affine = affine_fixed\n",
        "\n",
        "    # Registration\n",
        "    print('---Registration---')\n",
        "    t = time.time()\n",
        "    df, df_with_grid, warped_moving = registration(config, device, moving, fixed)\n",
        "    runtime = time.time() - t\n",
        "    print('Registration Running Time:', runtime)\n",
        "    print('---Registration DONE---')\n",
        "    print()\n",
        "\n",
        "    # Evaluate registration results and update the list of dice scores\n",
        "    print('---Evaluation---')\n",
        "    dice_list, neg_Jet = evaluation(config, device, df, df_with_grid, dice_list)\n",
        "    print('---Evaluation DONE---')\n",
        "    print()\n",
        "\n",
        "    # Save results\n",
        "    save_result(config, df, warped_moving)\n",
        "    print('---Results SAVED---')\n",
        "    print()\n",
        "\n",
        "    return dice_list\n",
        "\n",
        "def registration(config, device, moving, fixed):\n",
        "    '''\n",
        "    Registration moving to fixed.\n",
        "    :param config: configurations.\n",
        "    :param device: gpu or cpu.\n",
        "    :param moving: moving image to be registered, geodesic shooting starting point.\n",
        "    :param fixed: fixed image, geodesic shooting target.\n",
        "    :return best_df: displacement field at the end of training, without grid.\n",
        "    :return best_df_with_grid: displacement field at the end of training, with grid.\n",
        "    :return best_warped_moving: warped moving image at the end of training.\n",
        "    '''\n",
        "    # Image Shape (fixed.shape == moving.shape)\n",
        "    im_shape = fixed.shape\n",
        "\n",
        "    # Convert moving and fixed images to PyTorch tensors and move to specified device\n",
        "    moving = torch.from_numpy(moving).to(device).float()\n",
        "    fixed = torch.from_numpy(fixed).to(device).float()\n",
        "\n",
        "    # Make batch dimension\n",
        "    moving = moving.unsqueeze(0).unsqueeze(0)\n",
        "    fixed = fixed.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Initialize neural network\n",
        "    Network = BrainNet(img_sz=im_shape,\n",
        "                       smoothing_kernel=config.smoothing_kernel,\n",
        "                       smoothing_win=config.smoothing_win,\n",
        "                       smoothing_pass=config.smoothing_pass,\n",
        "                       ds=config.ds,\n",
        "                       bs=config.bs\n",
        "                       ).to(device)\n",
        "\n",
        "    # Initialize NeuralODE for solving the ODE\n",
        "    ode_train = NeuralODE(Network, config.optimizer, config.STEP_SIZE).to(device)\n",
        "\n",
        "    # Define scale factor, spatial transformer and grid\n",
        "    scale_factor = torch.tensor(im_shape).to(device).view(1, 3, 1, 1, 1) * 1.\n",
        "    ST = SpatialTransformer(im_shape).to(device)  # spatial transformer to warp image\n",
        "    grid = generate_grid3D_tensor(im_shape).unsqueeze(0).to(device)  # [-1,1]\n",
        "\n",
        "    # Define optimizer\n",
        "    optimizer = torch.optim.Adam(ode_train.parameters(), lr=config.lr, amsgrad=True)\n",
        "\n",
        "    # Initialize loss functions, df, df_with_grid, and warped_moving\n",
        "    loss_NCC = NCC(win=config.NCC_win)\n",
        "    BEST_loss_sim_loss_J = 1000\n",
        "    best_df = None\n",
        "    best_df_with_grid = None\n",
        "    best_warped_moving = None\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(config.epoches):\n",
        "        # Compute displacement and velocity field\n",
        "        all_phi = ode_train(grid, Tensor(np.arange(config.time_steps)), return_whole_sequence=True)\n",
        "        all_v = all_phi[1:] - all_phi[:-1]\n",
        "\n",
        "        # Convert displacement field to voxel spacing and warp moving image using the spatial transformer\n",
        "        all_phi = (all_phi + 1.) / 2. * scale_factor  # [-1, 1] -> voxel spacing\n",
        "        phi = all_phi[-1]\n",
        "        grid_voxel = (grid + 1.) / 2. * scale_factor  # [-1, 1] -> voxel spacing\n",
        "        df = phi - grid_voxel  # with grid -> without grid\n",
        "        warped_moving, df_with_grid = ST(moving, df, return_phi=True)\n",
        "\n",
        "        # Similarity loss\n",
        "        loss_sim = loss_NCC(warped_moving, fixed)\n",
        "        # Squeeze batch and channel dimensions from warped moving image\n",
        "        warped_moving = warped_moving.squeeze(0).squeeze(0)\n",
        "        # V magnitude loss\n",
        "        loss_v = config.lambda_v * magnitude_loss(all_v)\n",
        "        # Neg Jacobian loss\n",
        "        loss_J = config.lambda_J * neg_Jdet_loss(df_with_grid)\n",
        "        # Phi dphi/dx loss\n",
        "        loss_df = config.lambda_df * smoothloss_loss(df)\n",
        "        # Total Loss\n",
        "        loss = loss_sim + loss_v + loss_J + loss_df\n",
        "        \n",
        "        optimizer.zero_grad() # Zero out gradients from previous iteration\n",
        "        loss.backward() # Backpropagate the loss\n",
        "        optimizer.step() # Update the weights\n",
        "        \n",
        "        # Print loss information every 20 epochs\n",
        "        if (i + 1) % 20 == 0:\n",
        "            print(\"Iteration: {0} Loss_sim: {1:.3e} loss_J: {2:.3e}\".format(i + 1, loss_sim.item(), loss_J.item()))\n",
        "        \n",
        "        # Pick the one df with the most balanced loss_sim and loss_J in the last 50 epoches\n",
        "        if i > config.epoches - 50:\n",
        "            loss_sim_loss_J = 1000 * loss_sim.item() * loss_J.item() # Product of loss_sim and loss_J, scaled by 1000\n",
        "            if loss_sim_loss_J < BEST_loss_sim_loss_J: # If less than the current best loss, update values\n",
        "                best_df = df.detach().clone()\n",
        "                best_df_with_grid = df_with_grid.detach().clone()\n",
        "                best_warped_moving = warped_moving.detach().clone()\n",
        "    return best_df, best_df_with_grid, best_warped_moving\n",
        "\n",
        "\n",
        "def evaluation(config, device, df, df_with_grid, dice_list):\n",
        "    \"\"\"\n",
        "    Evaluation of registration by calculating negative Jacobian ratio and dice score.\n",
        "    :param config: configuration parameters\n",
        "    :param device: device to run the model on\n",
        "    :param df: displacement field without grid\n",
        "    :param df_with_grid: displacement field with grid\n",
        "    :param dice_list: list of dice scores\n",
        "    :return dice_res: mean dice score on given structures\n",
        "    \"\"\"\n",
        "    ### Calculate Neg Jac Ratio\n",
        "    neg_Jet = -1.0 * JacboianDet(df_with_grid)\n",
        "    neg_Jet = F.relu(neg_Jet)\n",
        "    mean_neg_J = torch.sum(neg_Jet).detach().cpu().numpy()\n",
        "    num_neg = len(torch.where(neg_Jet > 0)[0])\n",
        "    total = neg_Jet.size(-1) * neg_Jet.size(-2) * neg_Jet.size(-3)\n",
        "    ratio_neg_J = num_neg / total\n",
        "    print('Total of neg Jet: ', mean_neg_J)\n",
        "    print('Ratio of neg Jet: ', ratio_neg_J)\n",
        "\n",
        "    ### Calculate Dice\n",
        "    # Define the list of labels for structures to be evaluated\n",
        "\n",
        "    # labels for images provided by original NODEO paper:\n",
        "    label = [2, 3, 4, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 24, 28, 41, 42, 43, 46, 47, 49, 50, 51, 52, 53, 54, 60]\n",
        "\n",
        "    # labels for images of OASIS dataset downloaded from https://github.com/adalca/medical-datasets/blob/master/neurite-oasis.md\n",
        "    # label = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]\n",
        "\n",
        "    # Selected 27 labels from the labels above that correspond with the named labels mentioned in the paper.\n",
        "    # Only the structure 'CRF' could not be identified in the provided label set.(therefore only 27 instead of 28 labels)\n",
        "    #label = [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 33]\n",
        "\n",
        "    fixed_seg, _ = load_nii(config.fixed_seg)\n",
        "    moving_seg, _ = load_nii(config.moving_seg)\n",
        "    ST_seg = SpatialTransformer(fixed_seg.shape, mode='nearest').to(device)\n",
        "    moving_seg = torch.from_numpy(moving_seg).to(device).float()\n",
        "    # Make batch dimension\n",
        "    moving_seg = moving_seg[None, None, ...]\n",
        "    warped_seg = ST_seg(moving_seg, df, return_phi=False)\n",
        "    dice_move2fix = dice(warped_seg.unsqueeze(0).unsqueeze(0).detach().cpu().numpy(), fixed_seg, label)\n",
        "    dice_res = np.mean(dice_move2fix[0])\n",
        "    print('Avg. dice on %d structures: ' % len(label), dice_res)\n",
        "    return dice_res, neg_Jet\n",
        "\n",
        "def save_result(config, df, warped_moving):\n",
        "    \"\"\"\n",
        "    Save the displacement field and the warped moving image to disk.\n",
        "    \"\"\"\n",
        "    save_nii(df.permute(2,3,4,0,1).detach().cpu().numpy(), '%s_df.nii.gz' % (config.savepath), config.affine)\n",
        "    save_nii(warped_moving.detach().cpu().numpy(), '%s_warped.nii.gz' % (config.savepath), config.affine)\n",
        "\n",
        "def createConfig(id_fixed, id_moving):\n",
        "    \"\"\"\n",
        "    Configuration\n",
        "    \"\"\"\n",
        "    # Create Config\n",
        "    config = types.SimpleNamespace()\n",
        "\n",
        "    config.id_fixed = id_fixed # ID of fixed image\n",
        "    config.id_moving = id_moving # ID of moving image\n",
        "    config.affine = None # affine transformation matrix (assigned later after image has been loaded)\n",
        "\n",
        "    # File Paths\n",
        "    config.fixed = 'data/3D/OASIS_OAS1_0' + id_fixed + '_MR1/aligned_norm.nii.gz'\n",
        "    config.moving = 'data/3D/OASIS_OAS1_0' + id_moving + '_MR1/aligned_norm.nii.gz'\n",
        "    config.fixed_seg = 'data/3D/OASIS_OAS1_0' + id_fixed + '_MR1/aligned_seg35.nii.gz'\n",
        "    config.moving_seg = 'data/3D/OASIS_OAS1_0' + id_moving +  '_MR1/aligned_seg35.nii.gz'\n",
        "    config.savepath = 'result3d/' + config.id_fixed + config.id_moving\n",
        "\n",
        "    # Model Configuration\n",
        "    config.ds = 2 # Downsampling factor\n",
        "    config.bs = 16 # Batch Size\n",
        "    config.smoothing_kernel = 'AK' # Smoothing kernel type ('AK' (Averaging Kernel) or 'GK' (Gaussian Kernel))\n",
        "    config.smoothing_win = 15 # Smoothing window size\n",
        "    config.smoothing_pass = 1 # Number of smoothing passes\n",
        "\n",
        "    # Training Configuration\n",
        "    config.time_steps = 2\n",
        "    config.optimizer = 'Euler' # Solver type ('Euler' or 'RK' (4th Order Runge Kutta))\n",
        "    config.STEP_SIZE = 0.001\n",
        "    config.epoches = 300\n",
        "    config.lr = 0.005\n",
        "    config.loss_sim = 'NCC' # NCC (Normalized Cross Correlation)\n",
        "    config.NCC_win = 21\n",
        "    config.lambda_J = 2.5\n",
        "    config.lambda_df = 0.05\n",
        "    config.lambda_v = 0.00005\n",
        "\n",
        "    # Debug\n",
        "    config.debug = False\n",
        "    config.device = 'cuda:0'\n",
        "\n",
        "    return config\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Assign Image IDs\n",
        "    #fixed_ids = [1, 10, 20, 30, 40]\n",
        "    #moving_ids = [2, 3, 4, 5, 6, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 49]\n",
        "    \n",
        "    # For running a single image pair, adjust code like in the following two lines\n",
        "    fixed_ids = [1]\n",
        "    moving_ids = [2, 3]\n",
        "    \n",
        "    dice_list = [] # initialise list to store dice scores\n",
        "\n",
        "    for id_fixed in fixed_ids:\n",
        "      for id_moving in moving_ids:\n",
        "        print(\"-----------------------\")\n",
        "        print(\"NEW SET\")\n",
        "        print(\"Fixed Image ID: \" + str(id_fixed).zfill(3))\n",
        "        print(\"Moving Image ID: \" + str(id_moving).zfill(3))\n",
        "        print(\"-----------------------\")\n",
        "        print()\n",
        "        config = createConfig(str(id_fixed).zfill(3), str(id_moving).zfill(3))\n",
        "        dice_list.append(main(config, dice_list))\n",
        "\n",
        "    # Calculate mean dice score and standard deviation\n",
        "    mean_dice_score = np.mean(dice_list)\n",
        "    squared_diff = (dice_list - mean_dice_score) ** 2\n",
        "    variance = np.mean(squared_diff)\n",
        "    std_deviation = np.sqrt(variance)\n",
        "    \n",
        "    print(\"--------------\")\n",
        "    print(\"Dice Mean Scores Array: \" + str(dice_list))\n",
        "    print(\"Overall Mean Dice Score: \" + str(mean_dice_score))\n",
        "    print(\"Standard Deviation of Mean Dice Scores: \" + str(std_deviation))\n",
        "    print(\"--------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjyh9BXiqpa6",
        "outputId": "aa85b596-32c8-4d00-a38f-a4ad34c1ab2f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------\n",
            "NEW SET\n",
            "Fixed Image ID: 001\n",
            "Moving Image ID: 002\n",
            "-----------------------\n",
            "\n",
            "---Registration---\n",
            "Iteration: 20 Loss_sim: 7.226e-01 loss_J: 7.380e-04\n",
            "Iteration: 40 Loss_sim: 6.274e-01 loss_J: 4.059e-03\n",
            "Iteration: 60 Loss_sim: 6.095e-01 loss_J: 5.035e-03\n",
            "Iteration: 80 Loss_sim: 5.980e-01 loss_J: 3.627e-03\n",
            "Iteration: 100 Loss_sim: 5.903e-01 loss_J: 4.001e-03\n",
            "Iteration: 120 Loss_sim: 5.876e-01 loss_J: 3.432e-03\n",
            "Iteration: 140 Loss_sim: 5.835e-01 loss_J: 3.488e-03\n",
            "Iteration: 160 Loss_sim: 5.793e-01 loss_J: 4.565e-03\n",
            "Iteration: 180 Loss_sim: 5.777e-01 loss_J: 4.804e-03\n",
            "Iteration: 200 Loss_sim: 5.810e-01 loss_J: 3.027e-03\n",
            "Iteration: 220 Loss_sim: 5.754e-01 loss_J: 4.921e-03\n",
            "Iteration: 240 Loss_sim: 5.750e-01 loss_J: 4.776e-03\n",
            "Iteration: 260 Loss_sim: 5.728e-01 loss_J: 4.518e-03\n",
            "Iteration: 280 Loss_sim: 5.721e-01 loss_J: 3.510e-03\n",
            "Iteration: 300 Loss_sim: 5.718e-01 loss_J: 3.437e-03\n",
            "Registration Running Time: 306.6114180088043\n",
            "---Registration DONE---\n",
            "\n",
            "---Evaluation---\n",
            "Total of neg Jet:  116.48553\n",
            "Ratio of neg Jet:  0.0004293970412063163\n",
            "Avg. dice on 28 structures:  0.7911467356740279\n",
            "---Evaluation DONE---\n",
            "\n",
            "---Results SAVED---\n",
            "\n",
            "-----------------------\n",
            "NEW SET\n",
            "Fixed Image ID: 001\n",
            "Moving Image ID: 003\n",
            "-----------------------\n",
            "\n",
            "---Registration---\n",
            "Iteration: 20 Loss_sim: 7.937e-01 loss_J: 0.000e+00\n",
            "Iteration: 40 Loss_sim: 6.578e-01 loss_J: 4.216e-03\n",
            "Iteration: 60 Loss_sim: 6.234e-01 loss_J: 6.020e-04\n",
            "Iteration: 80 Loss_sim: 6.069e-01 loss_J: 1.305e-03\n",
            "Iteration: 100 Loss_sim: 5.988e-01 loss_J: 1.168e-03\n",
            "Iteration: 120 Loss_sim: 5.938e-01 loss_J: 9.428e-04\n",
            "Iteration: 140 Loss_sim: 5.880e-01 loss_J: 1.897e-03\n",
            "Iteration: 160 Loss_sim: 5.855e-01 loss_J: 9.749e-04\n",
            "Iteration: 180 Loss_sim: 5.824e-01 loss_J: 1.006e-03\n",
            "Iteration: 200 Loss_sim: 5.799e-01 loss_J: 1.862e-03\n",
            "Iteration: 220 Loss_sim: 5.816e-01 loss_J: 7.249e-04\n",
            "Iteration: 240 Loss_sim: 5.775e-01 loss_J: 9.090e-04\n",
            "Iteration: 260 Loss_sim: 5.757e-01 loss_J: 1.885e-03\n",
            "Iteration: 280 Loss_sim: 5.735e-01 loss_J: 1.340e-03\n",
            "Iteration: 300 Loss_sim: 5.748e-01 loss_J: 8.479e-04\n",
            "Registration Running Time: 306.8399510383606\n",
            "---Registration DONE---\n",
            "\n",
            "---Evaluation---\n",
            "Total of neg Jet:  2.5977306\n",
            "Ratio of neg Jet:  7.087709070805771e-06\n",
            "Avg. dice on 28 structures:  0.7872630129814365\n",
            "---Evaluation DONE---\n",
            "\n",
            "---Results SAVED---\n",
            "\n",
            "--------------\n",
            "Dice Mean Scores Array: [0.7911467356740279, 0.7872630129814365]\n",
            "Overall Mean Dice Score: 0.7892048743277322\n",
            "Standard Deviation of Mean Dice Scores: 0.0019418613462957035\n",
            "--------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "\n",
        "def getLabels(filepath):\n",
        "  img = nib.load(filepath)\n",
        "  data = img.get_fdata()\n",
        "  unique_labels = np.unique(data)\n",
        "  print(unique_labels)\n",
        "  print()\n",
        "\n",
        "print(\"Labels from nii.gz images provided by the paper:\")\n",
        "getLabels('data/3D/OAS1_0001_MR1/brain_aseg.nii.gz')\n",
        "\n",
        "print(\"Labels from GitHub OASIS dataset (3D):\")\n",
        "getLabels('data/3D/OASIS_OAS1_0001_MR1/aligned_seg35.nii.gz')\n",
        "\n",
        "print(\"Labels from GitHub OASIS dataset (2D):\")\n",
        "getLabels('data/2D/oasis/OASIS_OAS1_0001_MR1/slice_seg24.nii.gz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j_IA7lWNPB4",
        "outputId": "64a948aa-39d2-452c-d511-0655cdf97df8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels from nii.gz images provided by the paper:\n",
            "[ 0.  2.  3.  4.  5.  7.  8. 10. 11. 12. 13. 14. 15. 16. 17. 18. 24. 26.\n",
            " 28. 30. 41. 42. 43. 44. 46. 47. 49. 50. 51. 52. 53. 54. 58. 60. 62. 72.\n",
            " 78. 79. 81. 82. 85.]\n",
            "\n",
            "Labels from GitHub OASIS dataset (3D):\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.]\n",
            "\n",
            "Labels from GitHub OASIS dataset (2D):\n",
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 22. 23. 24.]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}